{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2CS SIQ2-SIL2 TP04. Na√Øve Bayes\n",
    "\n",
    "Dans ce TP, nous allons traiter Na√Øve Bayes. C'est le seul algorithme dans notre programme qui cr√©e un mod√®le g√©n√©ratif (en plus des auto-encodeurs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Bin√¥me 01** : \n",
    "- **Bin√¥me 02** :\n",
    "- **Groupe** : SIQ2|SIL2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1.21.5', '1.4.2', '3.5.1')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib\n",
    "import numpy             as np\n",
    "import pandas            as pd \n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "\n",
    "np        .__version__ , \\\n",
    "pd        .__version__ , \\\n",
    "matplotlib.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List, Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RAPPEL**\n",
    "\n",
    "Tout le monde connait le th√©or√®me de Bayes pour calculer la probabilit√© conditionnelle d'un √©vennement $A$ sachant un autre $B$ (si vous ne le connaissez pas; vous n'appartenez pas √† tout le monde) : \n",
    "$$ P(A|B) = \\frac{P(A)P(B|A)}{P(B)}$$\n",
    "\n",
    "Pour appliquer ce th√©or√®me sur un probl√®me d'appentissage automatique, l'id√©e est simple ; Etant donn√© une caract√©ristique $f$ et la sortie $y$ qui peut avoir la classe $c$ : \n",
    "- Remplacer $A$ par $y=c$\n",
    "- Remplacer $B$ par $f$ \n",
    "On aura l'√©quation : \n",
    "$$ P(y=c|f) = \\frac{P(y=c)P(f|y=c)}{P(f)}$$\n",
    "\n",
    "On appelle : \n",
    "- $P(y=c|f)$ post√©rieure \n",
    "- $P(y=c)$ ant√©rieure #la probabilit√© de la classe ùëê avant l'observation de la \n",
    "caract√©ristique ùëì. Elle peut √™tre estim√©e √† partir des donn√©es d'entra√Ænement en calculant simplement la proportion d'occurrences de chaque classe.\n",
    "- $P(f|y=c)$ vraisemblance #repr√©sente la probabilit√© que les caract√©ristiques observ√©es d'un fruit soient telles que le fruit appartienne √† une certaine classe (par exemple, une pomme ou une orange).par exemple, la probabilit√© que la couleur soit rouge sachant que le fruit est une pomme calculer √† partir des donn√©es de notre ensemble d'entra√Ænement (calcul√© pour chaque caract√©ristique).\n",
    "- $P(f)$ √©vidence  #repr√©sente la probabilit√© de voir les caract√©ristiques observ√©es dans notre ensemble de donn√©es, sans prendre en compte la classe des fruits. Dans notre exemple de fruits, l'√©vidence serait la probabilit√© de voir un fruit avec une certaine couleur et une certaine forme, quelle que soit sa classe.\n",
    "\n",
    "Ici, on estime la probablit√© d'une classe $c$ sachant une caract√©ristique $f$ en utilisant des donn√©es d'entrainement. Maintenant, on veut estimer la probabilit√© d'une classe $c$ sachant un vecteur de caract√©ristiques $\\overrightarrow{f} = \\{f_1, ..., f_L\\}$ : \n",
    "$$ P(y=c|\\overrightarrow{f}) = \\frac{P(y=c)P(\\overrightarrow{f}|y=c)}{P(f)}$$\n",
    "\n",
    "Etant donn√©e plusieurs classes $c_j$, la classe choisie $\\hat{c}$ est celle avec la probabilit√© maximale \n",
    "$$\\hat{c} = \\arg\\max\\limits_{c_k} P(y=c_k|\\overrightarrow{f})$$\n",
    "$$\\hat{c} = \\arg\\max\\limits_{c_k} \\frac{P(y=c_k)P(\\overrightarrow{f}|y=c_k)}{P(f)}$$\n",
    "On supprime l'√©vidence pour cacher le crime : $P(f)$ ne d√©pend pas de $c_k$ et elle est postive, donc √ßa ne va pas affecter la fonction $\\max$.\n",
    "$$\\hat{c} = \\arg\\max\\limits_{c_k} P(y=c_k)P(\\overrightarrow{f}|y=c_k)$$\n",
    "\n",
    "Pour calculer $P(\\overrightarrow{f}|y=c_k)$, on va utiliser une properi√©t√© na√Øve (d'o√π vient le nom Naive Bayes) : on suppose l'ind√©pendence conditionnelle entre les caract√©ristiques $f_j$. \n",
    "$$\\hat{c} = \\arg\\max\\limits_{c_k} P(y=c_k) \\prod\\limits_{f_j \\in \\overrightarrow{f}} P(f_j|y=c_k)$$\n",
    "\n",
    "Pour √©viter la disparition de la probabilit√© (multiplication et repr√©sentation de virgule flottante sur machine), on transforme vers l'espace logarithme.\n",
    "$$\\hat{c} = \\arg\\max\\limits_{c_k} \\log P(y=c_k) + \\sum\\limits_{f_j \\in \\overrightarrow{f}} \\log P(f_j|y=c_k)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. R√©alisation des algorithmes\n",
    "\n",
    "Pour estimer la vraisemblance, il existe plusieurs mod√®les (lois):\n",
    "- **Loi multinomiale :** pour les carac√©tristiques nominales\n",
    "- **Loi de Bernoulli :** lorsqu'on est interress√© par l'apparence d'une caract√©ristique ou non (binaire)\n",
    "- **Loi normale :** pour les caract√©ristiques num√©riques\n",
    "\n",
    "Dans ce TP, nous allons impl√©menter Naive Bayes pour les caract√©ristiques nominales (loi multinomiale). \n",
    "Dans notre mod√®le, nous voulons stocker les statistiques et pas les probabilit√©s. \n",
    "L'int√©r√™t est de faciliter la mise √† jours des statistiques (si par exemple, nous avons un autre dataset et nous voulons enrichir le mod√®le ; dans e cas, il suffit d'ajouter les statistiques du nouveau dataset)\n",
    "\n",
    "Ici, nous allons utiliser le dataset \"jouer\" (utilis√© dans la plupart des cours) contenant des caract√©ristiques nominales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>temps</th>\n",
       "      <th>temperature</th>\n",
       "      <th>humidite</th>\n",
       "      <th>vent</th>\n",
       "      <th>jouer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ensoleile</td>\n",
       "      <td>chaude</td>\n",
       "      <td>haute</td>\n",
       "      <td>non</td>\n",
       "      <td>non</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ensoleile</td>\n",
       "      <td>chaude</td>\n",
       "      <td>haute</td>\n",
       "      <td>oui</td>\n",
       "      <td>non</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nuageux</td>\n",
       "      <td>chaude</td>\n",
       "      <td>haute</td>\n",
       "      <td>non</td>\n",
       "      <td>oui</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pluvieux</td>\n",
       "      <td>douce</td>\n",
       "      <td>haute</td>\n",
       "      <td>non</td>\n",
       "      <td>oui</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pluvieux</td>\n",
       "      <td>fraiche</td>\n",
       "      <td>normale</td>\n",
       "      <td>non</td>\n",
       "      <td>oui</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>pluvieux</td>\n",
       "      <td>fraiche</td>\n",
       "      <td>normale</td>\n",
       "      <td>oui</td>\n",
       "      <td>non</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>nuageux</td>\n",
       "      <td>fraiche</td>\n",
       "      <td>normale</td>\n",
       "      <td>oui</td>\n",
       "      <td>oui</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ensoleile</td>\n",
       "      <td>douce</td>\n",
       "      <td>haute</td>\n",
       "      <td>non</td>\n",
       "      <td>non</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ensoleile</td>\n",
       "      <td>fraiche</td>\n",
       "      <td>normale</td>\n",
       "      <td>non</td>\n",
       "      <td>oui</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>pluvieux</td>\n",
       "      <td>douce</td>\n",
       "      <td>normale</td>\n",
       "      <td>non</td>\n",
       "      <td>oui</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ensoleile</td>\n",
       "      <td>douce</td>\n",
       "      <td>normale</td>\n",
       "      <td>oui</td>\n",
       "      <td>oui</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>nuageux</td>\n",
       "      <td>douce</td>\n",
       "      <td>haute</td>\n",
       "      <td>oui</td>\n",
       "      <td>oui</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>nuageux</td>\n",
       "      <td>chaude</td>\n",
       "      <td>normale</td>\n",
       "      <td>non</td>\n",
       "      <td>oui</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>pluvieux</td>\n",
       "      <td>douce</td>\n",
       "      <td>haute</td>\n",
       "      <td>oui</td>\n",
       "      <td>non</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        temps temperature humidite vent jouer\n",
       "0   ensoleile      chaude    haute  non   non\n",
       "1   ensoleile      chaude    haute  oui   non\n",
       "2     nuageux      chaude    haute  non   oui\n",
       "3    pluvieux       douce    haute  non   oui\n",
       "4    pluvieux     fraiche  normale  non   oui\n",
       "5    pluvieux     fraiche  normale  oui   non\n",
       "6     nuageux     fraiche  normale  oui   oui\n",
       "7   ensoleile       douce    haute  non   non\n",
       "8   ensoleile     fraiche  normale  non   oui\n",
       "9    pluvieux       douce  normale  non   oui\n",
       "10  ensoleile       douce  normale  oui   oui\n",
       "11    nuageux       douce    haute  oui   oui\n",
       "12    nuageux      chaude  normale  non   oui\n",
       "13   pluvieux       douce    haute  oui   non"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jouer   = pd.read_csv('Desktop/2Sem/ML/TPs/TP04/TP04/data/jouer.csv')\n",
    "\n",
    "X_jouer = jouer.iloc[:, :-1].values # Premi√®res colonnes \n",
    "Y_jouer = jouer.iloc[:,  -1].values # Derni√®re colonne \n",
    "\n",
    "# Afficher le dataset \"jouer\"\n",
    "jouer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.1. Entra√Ænement de la probabilit√© ant√©rieure\n",
    "\n",
    "Etant donn√© le vecteur de sortie $Y$, la probabilit√© de chaque classe \"oui ou bien non\" (diff√©rentes valeurs de $Y$) est calul√©e comme :\n",
    "\n",
    "$$p(c_k) = \\frac{|\\{y / y \\in Y \\text{ et } y = c_k\\}|}{|Y|}$$\n",
    "\n",
    "\n",
    "La fonction doit r√©cup√©rer des statistiques afin de pouvoir calculer la probabilit√© ant√©rieure de chaque classe. Donc, elle doit retourner  :\n",
    "- Un vecteur contenant les noms des classes\n",
    "- Un vecteur contenant les nombres d'occurrences de chaque classe dans le premier vecteur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['non', 'oui'], dtype=object), array([5, 9]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Stastistiques sur la probabilit√© ant√©rieure\n",
    "def stat_anterieure(Y: np.ndarray) -> Tuple[np.ndarray, np.ndarray]: \n",
    "    cls  = np.unique(Y) # le vecteur des classes\n",
    "    freq = []\n",
    "    for c in cls:\n",
    "        freq_c = len(Y[Y == c])\n",
    "        freq.append(freq_c) \n",
    "    return cls, np.array(freq)\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# (array(['non', 'oui'], dtype=object), array([5, 9]))\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "stat_anterieure(Y_jouer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.2. Entra√Ænement de la probabilit√© de vraissemblance (loi multinomiale)\n",
    "\n",
    "Notre mod√®le doit garder le nombre des diff√©rentes valeurs d'une caract√©ristique $A$ et le nombre de ces valeurs dans chaque classe.\n",
    "Donc, √©tant donn√© un vecteur d'une caract√©ristique $A= X[:,j]$, un autre des $Y$ et un $C$ contenant la liste des classes, la fonction d'entra√Ænement doit retourner : \n",
    "- $V$ : un vecteur contenant les diff√©rentes cat√©gories de $A$ (c'est d√©j√† fait)\n",
    "- Une matrice contenant le nombre d'occurrences de chaque cat√©gorie de $V$ dans chaque classe  : \n",
    "   - Les lignes repr√©sentent les cat√©gories $v \\in V$ de la car√©ct√©ristique $A$\n",
    "   - Les colonnes repr√©sentent les classes $c \\in C$ de $Y$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['ensoleile', 'nuageux', 'pluvieux'], dtype=object),\n",
       " array([[3., 2.],\n",
       "        [0., 4.],\n",
       "        [2., 3.]]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Statistiques de vraissemblance (une seule caract√©ristique)\n",
    "def stat_vraissemblance_1(A: np.ndarray, \n",
    "                          Y: np.ndarray, \n",
    "                          C: np.ndarray\n",
    "                         ) -> Tuple[np.ndarray, np.ndarray]: \n",
    "    V = np.unique(A) # Cat√©gories de la caract√©ristique A\n",
    "    freq = np.zeros((len(V), len(C)))  # Initialize freq with zeros\n",
    "    for j, c in enumerate(C):\n",
    "        for i, v in enumerate(V):\n",
    "            freq_c_v = len(A[(Y == c) & (A == v)]) # Fr√©quence de la cat√©gorie v de la caract√©ristique A pour la classe c\n",
    "            freq[i, j] = freq_c_v\n",
    "    return V, freq\n",
    "\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# (array(['ensoleile', 'nuageux', 'pluvieux'], dtype=object),\n",
    "#  array([[3, 2],\n",
    "#         [0, 4],\n",
    "#         [2, 3]]))\n",
    "#---------------------------------------------------------------------\n",
    "C_t = np.array(['non', 'oui'])\n",
    "stat_vraissemblance_1(X_jouer[:, 0], Y_jouer, C_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.3. Entra√Ænement loi multinomiale\n",
    "\n",
    "**Rien √† programmer ici**\n",
    "\n",
    "Notre mod√®le ($\\theta_{X, C}$) doit garder des statistiques sur les classes et aussi sur chaque cat√©gorie de chaque caract√©ristique. Pour ce faire, nous allons repr√©senter $\\theta$ comme un vecteur : \n",
    "- $\\theta[N+1]$ est un vecteur de $N$ √©l√©ments repr√©sentant des statistiques sur chaque caract√©ristique $j$, plus un √©l√©ment (le dernier) pour les statistiques sur les classes.\n",
    "- Chaque √©l√©ment est un dictionnaire (HashMap en Java)\n",
    "- Un √©l√©ment des caract√©ristiques contient deux cl√©s : \n",
    "    - **val** : pour r√©cup√©rer la liste des noms des cat√©gories de la caract√©ristique\n",
    "    - **freq**: pour r√©cup√©rer une matrice repr√©sentant la fr√©quence de chaque caract√©ristique dans chaque classe\n",
    "- Un √©l√©ment des classes contient deux cl√©s : \n",
    "    - **cls** : pour r√©cup√©rer la liste des noms des classes\n",
    "    - **freq**: pour r√©cup√©rer la liste des fr√©quences de chaque classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'val': array(['ensoleile', 'nuageux', 'pluvieux'], dtype=object),\n",
       "  'freq': array([[3., 2.],\n",
       "         [0., 4.],\n",
       "         [2., 3.]])},\n",
       " {'val': array(['chaude', 'douce', 'fraiche'], dtype=object),\n",
       "  'freq': array([[2., 2.],\n",
       "         [2., 4.],\n",
       "         [1., 3.]])},\n",
       " {'val': array(['haute', 'normale'], dtype=object),\n",
       "  'freq': array([[4., 3.],\n",
       "         [1., 6.]])},\n",
       " {'val': array(['non', 'oui'], dtype=object),\n",
       "  'freq': array([[2., 6.],\n",
       "         [3., 3.]])},\n",
       " {'cls': array(['non', 'oui'], dtype=object), 'freq': array([5, 9])}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# La fonction qui entraine Th√©ta sur plusieurs caract√©ristiques\n",
    "# Rien √† programmer ici\n",
    "# Notre th√©ta est une liste des dictionnaires;\n",
    "# chaque dictionnaire contient la liste des cat√©gories et la matrice des fr√©quences dela caract√©ristique respective √† la colonne de X\n",
    "# On ajoute les statistiques ant√©rieures des classes √† la fin de r√©sultat\n",
    "def entrainer_multi(X: np.ndarray, \n",
    "                    Y: np.ndarray\n",
    "                   ) -> np.ndarray: \n",
    "    \n",
    "    Theta   = []\n",
    "    \n",
    "    stats_c = {}\n",
    "    stats_c['cls'], stats_c['freq'] =  stat_anterieure(Y)\n",
    "    \n",
    "    for j in range(X.shape[1]): \n",
    "        stats = {}\n",
    "        stats['val'], stats['freq'] =  stat_vraissemblance_1(X[:, j], Y, stats_c['cls'])\n",
    "        Theta.append(stats)\n",
    "    \n",
    "    Theta.append(stats_c)\n",
    "    return Theta\n",
    "\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# [{'val': array(['ensoleile', 'nuageux', 'pluvieux'], dtype=object),\n",
    "#   'freq': array([[3, 2],\n",
    "#          [0, 4],\n",
    "#          [2, 3]])},\n",
    "#  {'val': array(['chaude', 'douce', 'fraiche'], dtype=object),\n",
    "#   'freq': array([[2, 2],\n",
    "#          [2, 4],\n",
    "#          [1, 3]])},\n",
    "#  {'val': array(['haute', 'normale'], dtype=object),\n",
    "#   'freq': array([[4, 3],\n",
    "#          [1, 6]])},\n",
    "#  {'val': array(['non', 'oui'], dtype=object),\n",
    "#   'freq': array([[2, 6],\n",
    "#          [3, 3]])},\n",
    "#  {'cls': array(['non', 'oui'], dtype=object), 'freq': array([5, 9])}]\n",
    "#---------------------------------------------------------------------\n",
    "Theta_jouer = entrainer_multi(X_jouer, Y_jouer)\n",
    "\n",
    "Theta_jouer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.4. Estimation de la probabilit√© de vraissemblance (loi multinomiale)\n",
    "L'√©quation pour estimer la vraisemblance \n",
    "$$ P(X_j=v|y=c_k) = \\frac{|\\{ y \\in Y / y = c_k \\text{ et } X_j = v\\}|}{|\\{y = c_k\\}|}$$\n",
    "\n",
    "\n",
    "Dans le cas d'une valeur $v$ qui n'existe pas dans le dataset d'entrainnement ou qui n'existe pas pour une classe donn√©e mais ui existe dans le dataset de test, nous aurons une probabilit√© nulle. \n",
    "Afin de r√©gler ce probl√®me, nous pouvons appliquer une fonction de lissage qui attribue une petite probabilit√© aux donn√©es non vues dans l'entra√Ænement. \n",
    "Le lissage que nous allons utiliser est celui de Lidstone. \n",
    "Lorsque $\\alpha = 1$, il est appel√© lissage de Laplace.\n",
    "$$ P(X_j=v|y=c_k) = \\frac{|\\{ y \\in Y / y = c_k \\text{ et } X_j = v\\}| + \\alpha}{|\\{y = c_k\\}| + \\alpha * |V|}$$\n",
    "O√π: \n",
    "- $\\alpha$ est une valeur donn√©e \n",
    "- $V$ est l'ensemble des diff√©rentes valeurs de $f_j$ (le vocabulaire; les cat√©gories)\n",
    "\n",
    "Etant donn√© : \n",
    "- $\\theta_j$ les param√®tres de la caract√©ristique $j$ repr√©sent√©es comme dictionnaire\n",
    "    - **val** : pour r√©cup√©rer la liste des noms des cat√©gories de la caract√©ristique (vocabulaire $V$)\n",
    "    - **freq**: pour r√©cup√©rer une matrice repr√©sentant la fr√©quence de chaque caract√©ristique dans chaque classe. C'est une matrice $|V|\\times|C|$\n",
    "- $v$ la valeur de la caract√©ristique $j$ utilis√©e pour calculer les probabilit√©s\n",
    "- $\\theta_c$ les param√®tres des classes $C$ repr√©sent√©es comme dictionnaire\n",
    "    - **cls** : pour r√©cup√©rer la liste des noms des classes\n",
    "    - **freq**: pour r√©cup√©rer la liste des fr√©quences des classes\n",
    "    \n",
    "Cette fonction doit retourner : \n",
    "- Une liste $P[|C|]$ contenant les probabilit√©s de la cat√©gorie $v$ de $X_j$ sur toutes les classes $C$ \n",
    "- Elle doit prendre en consid√©ration le cas o√π la valeur $v$ n'existe pas dans le mod√®le entra√Æn√©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.4       , 0.33333333]), array([0.125     , 0.08333333]))"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def P_vraiss_multi(Theta_j: Dict[str, np.ndarray], \n",
    "                   Theta_c: Dict[str, np.ndarray], \n",
    "                   v: str, \n",
    "                   alpha: float = 0.\n",
    "                  ) -> np.ndarray:\n",
    "        \n",
    "    # une liste des indices o√π se trouve la valeur v dans Theta_j[\"val\"]\n",
    "    ind = np.where(Theta_j['val'] == v)[0] \n",
    "    \n",
    "    # Get the vocabulary and frequency matrix of feature j\n",
    "    vocab_j = Theta_j[\"val\"] \n",
    "    freq_j = Theta_j[\"freq\"]\n",
    "    \n",
    "    # Get the classes their frequency\n",
    "    classes = Theta_c[\"cls\"]\n",
    "    freq_c = Theta_c[\"freq\"]\n",
    "    \n",
    "    if len(ind) == 0:\n",
    "         default_prob = alpha / (freq_c + alpha * len(vocab_j))\n",
    "         return default_prob\n",
    "    \n",
    "    prob_v = (freq_j[ind,:]  / freq_c )[0]\n",
    "    \n",
    "    return prob_v\n",
    "\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# (array([0.4       , 0.33333333]), array([0.125     , 0.08333333]))\n",
    "#---------------------------------------------------------------------\n",
    "# Calcul :\n",
    "# La probabilit√© de jouer si temps = pluvieux \n",
    "# P(temps = pluvieux | jouer=oui) = (nbr(temps=pluvieux et jouer=oui)+alpha)/(nbr(jour=oui) + alpha * nbr_diff(temps)))\n",
    "# P(temps = pluvieux | jouer=oui) = (3 + 0)/(9 + 0) ==> 3 est le nombre de diff√©rentes valeurs de temps (entrainnement)\n",
    "# P(temps = pluvieux | jouer=oui) = 4/12 ==> 0.33333333333333333333333333333333333~\n",
    "\n",
    "# La probabilit√© de jouer si temps = neigeux \n",
    "# P(temps = neigeux | jouer=oui) = (nbr(temps=neigeux et jouer=oui)+alpha)/(nbr(jouer=oui) + alpha * nbr_diff(temps)))\n",
    "# P(temps = neigeux | jouer=oui) = (0 + 1)/(9 + 3) ==> 3 est le nombre de diff√©rentes valeurs de temps (entrainnement)\n",
    "# P(temps = neigeux | jouer=oui) = 1/13 ==> 0.0833333333333333333333333333333333333~\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "P_vraiss_multi(Theta_jouer[0], Theta_jouer[-1], 'pluvieux'), \\\n",
    "P_vraiss_multi(Theta_jouer[0], Theta_jouer[-1], 'neigeux', alpha=1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.5. Pr√©diction de la classe (loi multinomiale)\n",
    "Revenons maintenant √† notre √©quation de pr√©diction \n",
    "$$\\hat{c} = \\arg\\max\\limits_{c_k} \\log P(y=c_k) + \\sum\\limits_{f_j \\in \\overrightarrow{f}} \\log P(f_j|y=c_k)$$\n",
    "\n",
    "- On doit pr√©dire un seule √©chantillon $x$. \n",
    "- La fonction doit retourner un vecteur des log-probabilit√© des classes\n",
    "- Si anter=false donc on n'utilise pas la probabilit√© ant√©rieure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [149]\u001b[0m, in \u001b[0;36m<cell line: 38>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m log_probs\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# TEST UNITAIRE\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m#=====================================================================\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Resultat : \u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# (array([-5.20912179, -4.10264337]), array([-4.17950237, -3.66081061]))\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m#---------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m \u001b[43mpredire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpluvieux\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfraiche\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnormale\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moui\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTheta_jouer\u001b[49m\u001b[43m)\u001b[49m, \\\n\u001b[0;32m     39\u001b[0m predire([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpluvieux\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfraiche\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnormale\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moui\u001b[39m\u001b[38;5;124m'\u001b[39m], Theta_jouer, anter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Input \u001b[1;32mIn [149]\u001b[0m, in \u001b[0;36mpredire\u001b[1;34m(x, Theta, alpha, anter)\u001b[0m\n\u001b[0;32m     27\u001b[0m     freq_k \u001b[38;5;241m=\u001b[39m Theta[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfreq\u001b[39m\u001b[38;5;124m\"\u001b[39m][k]\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;66;03m# Calculate the log-probability of class k given feature x\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m     log_probs[k] \u001b[38;5;241m=\u001b[39m log_prob_k \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(freq_k) \u001b[38;5;241m+\u001b[39m prob_xj\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m log_probs\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "# TODO: Pr√©diction des log des probabilit√©s\n",
    "def predire(x: List[str], Theta: List[Dict[str, np.ndarray]], alpha: float = 1., anter: bool = True) -> np.ndarray:\n",
    "    # Get the number of classes\n",
    "    K = Theta[-1][\"cls\"].size\n",
    "\n",
    "    # Initialize the array of log-probabilities\n",
    "    log_probs = np.zeros(K)\n",
    "\n",
    "    # Calculate the log-probability of each class k\n",
    "    log_prob_k = np.log(Theta[-1][\"freq\"]) if anter else 0.\n",
    "\n",
    "    # Calculate the sum of log-probabilities of features for class k\n",
    "    for k in range(K):\n",
    "        prob_xj = 0\n",
    "\n",
    "        for j in range(len(x)):\n",
    "            # Get the vocabulary and frequency matrix of feature j \n",
    "            Theta_j = Theta[j]\n",
    "\n",
    "            # Get the number of categories for feature j\n",
    "            J = Theta_j[\"val\"].shape[0]\n",
    "\n",
    "            # Calculate the probability of feature x_j given class k\n",
    "            prob_xj += np.log(P_vraiss_multi(Theta_j, Theta[-1], x[j], alpha)[k])\n",
    "\n",
    "        # Get the frequency of class k\n",
    "        freq_k = Theta[-1][\"freq\"][k]\n",
    "\n",
    "        # Calculate the log-probability of class k given feature x\n",
    "        log_probs[k] = log_prob_k + np.log(freq_k) + prob_xj\n",
    "\n",
    "    return log_probs\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# (array([-5.20912179, -4.10264337]), array([-4.17950237, -3.66081061]))\n",
    "#---------------------------------------------------------------------\n",
    "predire(['pluvieux', 'fraiche', 'normale', 'oui'], Theta_jouer), \\\n",
    "predire(['pluvieux', 'fraiche', 'normale', 'oui'], Theta_jouer, anter=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.6. Regrouper en une classe (loi multinomiale)\n",
    "\n",
    "**Rien √† programmer ici**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['oui', 'non']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NBMultinom(object): \n",
    "    \n",
    "    def __init__(self, alpha=1.): \n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def entrainer(self, X, Y):\n",
    "        self.Theta = entrainer_multi(X, Y)\n",
    "    \n",
    "    def predire(self, X, anter=True, prob=False): \n",
    "        Y_pred = []\n",
    "        cls = self.Theta[-1]['cls']\n",
    "        for i in range(len(X)): \n",
    "            log_prob = predire(X[i,:], self.Theta, alpha=self.alpha, anter=anter)\n",
    "            if prob:\n",
    "                Y_pred.append(np.max(log_prob))\n",
    "            else:\n",
    "                Y_pred.append(cls[np.argmax(log_prob)])\n",
    "        return Y_pred\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# ['oui', 'non']\n",
    "#---------------------------------------------------------------------\n",
    "notre_modele = NBMultinom()\n",
    "notre_modele.entrainer(X_jouer, Y_jouer)\n",
    "X_test = np.array([\n",
    "    ['neigeux', 'fraiche', 'normale', 'oui'],\n",
    "    ['neigeux', 'fraiche', 'haute'  , 'oui']\n",
    "])\n",
    "notre_modele.predire(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Application et analyse\n",
    "\n",
    "**Il n'y a rien √† programmer ici.**\n",
    "\n",
    "Le but de cette section est de mener des exp√©rimentations afin de bien comprendre les concepts vus dans le cours.\n",
    "Aussi, elle nous assiste √† comprendre l'effet des diff√©rents param√®tres.\n",
    "En plus, la discussion des diff√©rentes exp√©rimentations peut am√©liorer l'aspect analytique chez l'√©tudient.\n",
    "\n",
    "### II.1. Probabilit√© ant√©rieure \n",
    "\n",
    "Nous voulons tester l'effet de la probabilit√© ant√©rieure.\n",
    "Pour ce faire, nous avons entra√Æn√© deux mod√®les :\n",
    "1. Avec probabilit√© ant√©rieure\n",
    "1. Sans probabilit√© ant√©rieure (Il consid√®re une distribution uniforme des classes)\n",
    "\n",
    "Pour tester si les mod√®les ont bien s'adapter au dataset d'entra√Ænement, nous allons les tester sur le m√™me dataset et calculer le rapport de classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avec probabilit√© ant√©rieure (a priori)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         non       1.00      0.80      0.89         5\n",
      "         oui       0.90      1.00      0.95         9\n",
      "\n",
      "    accuracy                           0.93        14\n",
      "   macro avg       0.95      0.90      0.92        14\n",
      "weighted avg       0.94      0.93      0.93        14\n",
      "\n",
      "Sans probabilit√© ant√©rieure (a priori)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         non       0.67      0.80      0.73         5\n",
      "         oui       0.88      0.78      0.82         9\n",
      "\n",
      "    accuracy                           0.79        14\n",
      "   macro avg       0.77      0.79      0.78        14\n",
      "weighted avg       0.80      0.79      0.79        14\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# AVEC Scikit-learn\n",
    "# ===================\n",
    "from sklearn.naive_bayes   import CategoricalNB\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.metrics       import classification_report\n",
    "\n",
    "nb_avec     = CategoricalNB(alpha=1.0, fit_prior=True )\n",
    "nb_sans     = CategoricalNB(alpha=1.0, fit_prior=False)\n",
    "\n",
    "enc         = OrdinalEncoder()\n",
    "X_jouer_enc = enc.fit_transform(X_jouer)\n",
    "nb_avec.fit(X_jouer_enc, Y_jouer)\n",
    "nb_sans.fit(X_jouer_enc, Y_jouer)\n",
    "\n",
    "Y_pred_avec = nb_avec.predict(X_jouer_enc)\n",
    "Y_pred_sans = nb_sans.predict(X_jouer_enc)\n",
    "\n",
    "# AVEC notre mod√®le (juste pour voir comment l'utiliser)\n",
    "# =======================================================\n",
    "#notre_modele = NBMultinom()\n",
    "#notre_modele.entrainer(X_jouer, Y_jouer)\n",
    "#Y_notre_ant = notre_modele.predire(X_jouer)\n",
    "#Y_notre_sans_ant = notre_modele.predire(X_jouer, anter=False) \n",
    "\n",
    "# Le rapport de classification\n",
    "\n",
    "\n",
    "print( 'Avec probabilit√© ant√©rieure (a priori)'  )\n",
    "print(classification_report(Y_jouer, Y_pred_avec))\n",
    "\n",
    "print( 'Sans probabilit√© ant√©rieure (a priori)'  )\n",
    "print(classification_report(Y_jouer, Y_pred_sans))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: Analyser les r√©sultats**\n",
    "    \n",
    "- Que remarquez-vous ?\n",
    "- Est-ce que la probabilit√© ant√©rieure est importante dans ce cas ?\n",
    "- Comment cette probabilit√© affecte le r√©sultat ?\n",
    "- Quand est-ce que nous sommes s√ªrs que l'utilisation de cette probabilit√© est inutile ?\n",
    "\n",
    "**R√©ponse**\n",
    "\n",
    "- En analysant les r√©sultats, on peut remarquer que la pr√©cision, le rappel et le score F1  et l'accuracy globale et meme les valeurs de macro avg et weighted avg sont meilleurs pour le mod√®le utilisant la probabilit√© ant√©rieure que pour celui sans probabilit√© ant√©rieure.\n",
    "\n",
    "- Oui, la probabilit√© ant√©rieure est importante dans ce cas. En effet, sans la probabilit√© ant√©rieure, le mod√®le consid√®re une distribution uniforme des classes, ce qui est un mauvais choix dans notre cas car les classes ne sont pas √©quitablement repr√©sent√©es dans ce jeu de donn√©es (5 pour la classe \"non\" et 9 pour la classe \"oui\") ,Dans notre exemple, Lorsque nous utilisons la probabilit√© ant√©rieure, le mod√®le prend en compte ces fr√©quences dans la phase d'apprentissage et ajuste les param√®tres en cons√©quence. Ainsi, le mod√®le sera mieux √©quip√© pour traiter les cas o√π les classes sont d√©s√©quilibr√©es,De plus le fait que le nombre d'exemples pour chaque classe soit petit peut rendre difficile l'apprentissage des mod√®les sans la probabilit√© ant√©rieure.\n",
    "\n",
    "- Lorsque nous utilisons la probabilit√© ant√©rieure (a priori), le mod√®le suppose que toutes les classes ont une probabilit√© diff√©rente, en fonction de leur fr√©quence dans l'ensemble des donn√©es d'entra√Ænement. En revanche, lorsque nous n'utilisons pas la probabilit√© ant√©rieure, le mod√®le suppose que toutes les classes ont une probabilit√© √©gale .Donc, l'effet de la probabilit√© ant√©rieure d√©pend de la distribution des classes dans l'ensemble des donn√©es d'entra√Ænement. Si une classe est sous-repr√©sent√©e dans l'ensemble des donn√©es d'entra√Ænement, l'utilisation de la probabilit√© ant√©rieure peut aider √† corriger cette disproportion en augmentant la probabilit√© de cette classe. Si toutes les classes sont repr√©sent√©es de mani√®re √©quivalente dans l'ensemble des donn√©es d'entra√Ænement, l'utilisation de la probabilit√© ant√©rieure peut ne pas avoir beaucoup d'impact sur les estimations de probabilit√© du mod√®le.\n",
    "\n",
    "- En g√©n√©ral, l'utilisation de la probabilit√© ant√©rieure peut √™tre inutile lorsque les classes sont √©quitablement repr√©sent√©es dans le jeu de donn√©es, c'est-√†-dire lorsque chaque classe a approximativement la m√™me proportion d'exemples. Dans ce cas, l'utilisation de la probabilit√© ant√©rieure n'apportera pas de gain significatif en termes de performances et pourrait m√™me causer une baisse de celles-ci si la probabilit√© ant√©rieure choisie est mal adapt√©e."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.2. Lissage\n",
    "\n",
    "Nous voulons tester l'effet de lissage de Lidstone.\n",
    "Pour ce faire, nous avons entra√Æn√© trois mod√®les : \n",
    "1. alpha = 1 (lissage de Laplace)\n",
    "1. alpha = 0.5\n",
    "1. alpha = 0 (sans lissage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha = 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         non       1.00      0.80      0.89         5\n",
      "         oui       0.90      1.00      0.95         9\n",
      "\n",
      "    accuracy                           0.93        14\n",
      "   macro avg       0.95      0.90      0.92        14\n",
      "weighted avg       0.94      0.93      0.93        14\n",
      "\n",
      "Alpha = 0.5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         non       1.00      0.80      0.89         5\n",
      "         oui       0.90      1.00      0.95         9\n",
      "\n",
      "    accuracy                           0.93        14\n",
      "   macro avg       0.95      0.90      0.92        14\n",
      "weighted avg       0.94      0.93      0.93        14\n",
      "\n",
      "Alpha = 0.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         non       1.00      0.80      0.89         5\n",
      "         oui       0.90      1.00      0.95         9\n",
      "\n",
      "    accuracy                           0.93        14\n",
      "   macro avg       0.95      0.90      0.92        14\n",
      "weighted avg       0.94      0.93      0.93        14\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:555: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "NBC_10 = CategoricalNB(alpha = 1.0 )\n",
    "NBC_05 = CategoricalNB(alpha = 0.5 )\n",
    "NBC_00 = CategoricalNB(alpha = 0.0 )\n",
    "\n",
    "NBC_10.fit( X_jouer_enc,   Y_jouer )\n",
    "NBC_05.fit( X_jouer_enc,   Y_jouer )\n",
    "NBC_00.fit( X_jouer_enc,   Y_jouer )\n",
    "\n",
    "Y_10   = NBC_10.predict(X_jouer_enc)\n",
    "Y_05   = NBC_05.predict(X_jouer_enc)\n",
    "Y_00   = NBC_00.predict(X_jouer_enc)\n",
    "\n",
    "\n",
    "print(          'Alpha = 1.0'             )\n",
    "print(classification_report(Y_jouer, Y_10))\n",
    "\n",
    "print(          'Alpha = 0.5'             )\n",
    "print(classification_report(Y_jouer, Y_05))\n",
    "\n",
    "print(          'Alpha = 0.0'             )\n",
    "print(classification_report(Y_jouer, Y_00))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: Analyser les r√©sultats**\n",
    "\n",
    "- Que remarquez-vous ?\n",
    "- Est-ce que le lissage affecte la performance dans ce cas ? Pourquoi ?\n",
    "- Pourquoi Scikit-learn n'accepte pas la valeur $\\alpha=0$ et affiche une alerte \"UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\" ?\n",
    "- Quelle est l'int√©r√™t du lissage (dans le cas g√©n√©ral) ?\n",
    "\n",
    "**R√©ponse**\n",
    "\n",
    "- En analysant les r√©sultats, on remarque que les trois mod√®les donnent des performances similaires, avec une pr√©cision, un rappel et une mesure F1 √©lev√©s pour les deux classes ('non' et 'oui'), ainsi qu'une pr√©cision globale de 0,93.\n",
    "\n",
    "- Le lissage n'a pas beaucoup d'effet sur la performance de ce mod√®le dans ce cas particulier. Cela peut √™tre d√ª √† plusieurs facteurs, dont la taille de l'ensemble de donn√©es. Comme cet ensemble de donn√©es est petit, il peut y avoir peu de risques de sur-ajustement et donc le lissage n'a pas un effet significatif sur la performance du mod√®le. De plus, le nombre de caract√©ristiques et de classes dans cet ensemble de donn√©es est √©galement limit√©, ce qui r√©duit la complexit√© du mod√®le.\n",
    "\n",
    "- Dans la formule de Lidstone, on divise le nombre d'occurrences de chaque cat√©gorie par le nombre total d'occurrences, ce qui peut conduire √† une division par z√©ro si le nombre d'occurrences pour une cat√©gorie donn√©e est √©gal √† z√©ro. Pour √©viter cette erreur, on ajoute une petite quantit√© alpha √† chaque compteur de cat√©gorie. Si alpha est √©gal √† z√©ro, alors la division est impossible car on divise par z√©ro. Pour √©viter cette erreur, Scikit-learn utilise une valeur tr√®s petite pour alpha, telle que 1.0e-10, lorsqu'elle est proche de z√©ro. Cette valeur est g√©n√©ralement suffisamment petite pour √©viter les erreurs num√©riques tout en garantissant que l'ajout de cette petite quantit√© ne modifie pas significativement les estimations de probabilit√© pour les diff√©rentes classes.\n",
    "\n",
    "- En g√©n√©ral, l'int√©r√™t du lissage est de permettre une meilleure g√©n√©ralisation du mod√®le, en √©vitant que les estimations de probabilit√© pour certaines classes soient trop √©lev√©es ou trop basses, ce qui pourrait entra√Æner un sur-ajustement et une performance m√©diocre sur de nouvelles donn√©es. Le lissage aide √©galement √† √©viter les cas o√π une caract√©ristique donn√©e n'est pas pr√©sente dans la classe d'apprentissage, ce qui conduirait √† une estimation de probabilit√© nulle pour cette caract√©ristique. Dans ces cas-l√†, le lissage permet d'attribuer une petite probabilit√© √† la caract√©ristique, √©vitant ainsi une estimation de probabilit√© nulle et am√©liorant la performance du mod√®le."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.3. Comparaison avec d'autres algorithmes\n",
    "\n",
    "Naive Bayes est un algorithme puissant lorsqu'il s'agit de classer les documents textuels ; nous voulons tester cette information avec la d√©tection de spam. \n",
    "Le dataset utilis√© est [SMS Spam Collection Dataset](https://www.kaggle.com/uciml/sms-spam-collection-dataset).\n",
    "Chaque message du dataset doit √™tre repr√©sent√© sous forme d'un mod√®le \"Sac √† mots\" (BoW : Bag of Words).\n",
    "Dans l'entra√Ænement, les diff√©rents mots qui s'apparaissent dans les messages (vocabulaire) sont consid√©r√©s comme des caract√©ristiques. \n",
    "Donc, pour chaque message, la valeur de la caract√©ristique est la fr√©quence du mot dans le message. \n",
    "Par exemple, si le mot \"good\" apparait 3 fois dans le message, donc la caract√©ristique \"good\" aura la valeur 3 dans ce message.\n",
    "\n",
    "Notre impl√©mentation n'est pas ad√©quate pour la nature de ce probl√®me. \n",
    "Dans Scikit-learn, [sklearn.naive_bayes.CategoricalNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.CategoricalNB.html) est similaire √† notre impl√©mentation. \n",
    "L'algorithme ad√©quat pour ce type de probl√®me est [sklearn.naive_bayes.MultinomialNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html).\n",
    "Les algorithmes compar√©s :\n",
    "1. Naive Bayes (Loi Multinomiale)\n",
    "1. Naive Bayes (Loi Gaussienne)\n",
    "1. Regression logistique \n",
    "1. Arbres de decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texte</th>\n",
       "      <th>classe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               texte classe\n",
       "0  Go until jurong point, crazy.. Available only ...    ham\n",
       "1                      Ok lar... Joking wif u oni...    ham\n",
       "2  Free entry in 2 a wkly comp to win FA Cup fina...   spam\n",
       "3  U dun say so early hor... U c already then say...    ham\n",
       "4  Nah I don't think he goes to usf, he lives aro...    ham"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lire le dataset\n",
    "messages = pd.read_csv('Desktop/2Sem/ML/TPs/TP04/TP04/data/spam.csv', encoding='latin-1')\n",
    "# renomer les caract√©ristiques : texte et classe\n",
    "messages = messages.rename(columns={'v1': 'classe', 'v2': 'texte'})\n",
    "# garder seulement ces deux caract√©ristiques\n",
    "messages = messages.filter(['texte', 'classe'])\n",
    "\n",
    "messages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fin\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection         import train_test_split\n",
    "from sklearn.naive_bayes             import MultinomialNB, GaussianNB\n",
    "from sklearn.linear_model            import LogisticRegression\n",
    "from sklearn.tree                    import DecisionTreeClassifier\n",
    "from sklearn.metrics                 import precision_score, recall_score\n",
    "import timeit\n",
    "\n",
    "\n",
    "modeles = [\n",
    "    MultinomialNB(),\n",
    "    GaussianNB(),\n",
    "    LogisticRegression(solver='lbfgs') ,\n",
    "    #solver=sag est plus lent; donc j'ai choisi le plus rapide\n",
    "    DecisionTreeClassifier()\n",
    "]\n",
    "\n",
    "temps_train = []\n",
    "temps_test  = []\n",
    "rappel      = []\n",
    "precision   = []\n",
    "\n",
    "msg_train, msg_test, Y_train, Y_test = train_test_split(messages['texte'] ,\n",
    "                                                        messages['classe'],\n",
    "                                                        test_size    = 0.2, \n",
    "                                                        random_state = 0  )\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "X_train          = count_vectorizer.fit_transform(msg_train).toarray()\n",
    "X_test           = count_vectorizer.transform    (msg_test ).toarray()\n",
    "\n",
    "\n",
    "for modele in modeles:\n",
    "    # ==================================\n",
    "    # ENTRAINEMENT \n",
    "    # ==================================\n",
    "    temps_debut = timeit.default_timer()\n",
    "    modele.fit(X_train, Y_train)\n",
    "    temps_train.append(timeit.default_timer() - temps_debut)\n",
    "    \n",
    "    # ==================================\n",
    "    # TEST \n",
    "    # ==================================\n",
    "    temps_debut = timeit.default_timer()\n",
    "    Y_pred      = modele.predict(X_test)\n",
    "    temps_test.append(timeit.default_timer() - temps_debut)\n",
    "    \n",
    "    # ==================================\n",
    "    # PERFORMANCE \n",
    "    # ==================================\n",
    "    # Ici, nous consid√©rons une classification binaire avec une seule classe \"spam\" \n",
    "    # le classifieur ne sera pas jug√© par sa capacit√© de d√©tecter les non spams\n",
    "    precision.append(precision_score(Y_test, Y_pred, pos_label='spam'))\n",
    "    rappel   .append(recall_score   (Y_test, Y_pred, pos_label='spam'))\n",
    "\n",
    "    \n",
    "print('Fin') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II.3.1. Temps d'entra√Ænement et de test\n",
    "\n",
    "Combien de temps chaque algorithme prend pour entrainer le m√™me dataset d'entrainement et combien de temps pour tester le m√™me dataset de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Algorithme</th>\n",
       "      <th>Temps d'entrainement</th>\n",
       "      <th>Temps de test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Naive Bayes Multinomial (NBM)</td>\n",
       "      <td>0.267185</td>\n",
       "      <td>0.018101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Naive Bayes Gaussien (NBG)</td>\n",
       "      <td>0.318484</td>\n",
       "      <td>0.074441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Regression logistique (RL)</td>\n",
       "      <td>0.836053</td>\n",
       "      <td>0.016207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arbre de decision (AD)</td>\n",
       "      <td>8.721873</td>\n",
       "      <td>0.010284</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Algorithme  Temps d'entrainement  Temps de test\n",
       "0  Naive Bayes Multinomial (NBM)              0.267185       0.018101\n",
       "1     Naive Bayes Gaussien (NBG)              0.318484       0.074441\n",
       "2     Regression logistique (RL)              0.836053       0.016207\n",
       "3         Arbre de decision (AD)              8.721873       0.010284"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algo_noms = ['Naive Bayes Multinomial (NBM)', \n",
    "             'Naive Bayes Gaussien (NBG)'   , \n",
    "             'Regression logistique (RL)'   , \n",
    "             'Arbre de decision (AD)']\n",
    "\n",
    "pd.DataFrame({\n",
    "    'Algorithme'            : algo_noms  ,\n",
    "    'Temps d\\'entrainement' : temps_train,\n",
    "    'Temps de test'         : temps_test\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: Analyser les r√©sultats**\n",
    "\n",
    "- Que remarquez-vous concernant le temps d'entrainement ? (ordonner les algorithmes)\n",
    "- Pourquoi nous avons eu ces r√©sultats en se basant sur les algorithmes ? (discuter chaque algorithme vis-a-vis le temps d'entrainement)\n",
    "- Que remarquez-vous concernant le temps de test ? (ordonner les algorithmes)\n",
    "- Pourquoi nous avons eu ces r√©sultats en se basant sur les algorithmes ? (discuter chaque algorithme vis-a-vis le temps de test)\n",
    "\n",
    "**R√©ponse**\n",
    "\n",
    "- En examinant les r√©sultats, on peut remarquer que le temps d'entra√Ænement varie consid√©rablement selon l'algorithme utilis√©. L'arbre de d√©cision prend le plus de temps pour s'entra√Æner, suivi de la r√©gression logistique, du Naive Bayes Gaussien et enfin du Naive Bayes Multinomial. Par cons√©quent, le classement de temps d'entra√Ænement, du plus long au plus court, serait : arbre de d√©cision, r√©gression logistique, Naive Bayes Gaussien, Naive Bayes Multinomial.\n",
    "\n",
    "\n",
    "- Ces r√©sultats peuvent √™tre expliqu√©s en fonction de la complexit√© de l'algorithme. \n",
    "   - **Naive Bayes Multinomial** : L'entra√Ænement du mod√®le Naive Bayes Multinomial est relativement simple et rapide, car il suffit de compter le nombre d'occurrences de chaque mot dans chaque message. La repr√©sentation \"Sac √† mots\" est tr√®s efficace pour repr√©senter les donn√©es textuelles, car elle ne tient pas compte de l'ordre des mots dans les phrases, mais seulement de leur fr√©quence. Cette repr√©sentation permet de r√©duire le temps d'entra√Ænement, car le mod√®le est beaucoup moins complexe que d'autres mod√®les qui prennent en compte l'ordre des mots, tels que les r√©seaux de neurones.\n",
    "   - **Naive Bayes Gaussien** : contrairement au Naive Bayes Multinomial, le Naive Bayes Gaussien suppose que les caract√©ristiques suivent une distribution normale. Cela signifie que pour chaque classe, le mod√®le doit estimer la moyenne et la variance de chaque caract√©ristique. Pour ce faire, l'algorithme doit parcourir tout le dataset plusieurs fois pour estimer ces param√®tres. Par cons√©quent, le temps d'entrainement du Naive Bayes Gaussien est plus long que celui du Naive Bayes Multinomial.\n",
    "   - **Regression logistique** : Cette m√©thode est √©galement plus complexe que les deux pr√©cedentes car elle suppose une relation lin√©aire entre les caract√©ristiques et la variable cible. Pour entra√Æner le mod√®le, l'algorithme utilise une fonction de co√ªt qui mesure l'erreur de pr√©diction du mod√®le et ajuste les coefficients pour minimiser cette erreur. Ce processus d'optimisation n√©cessite des calculs it√©ratifs pour trouver les coefficients optimaux qui minimisent la fonction de co√ªt.\n",
    "   - **Arbre de d√©cision** : Cet algorithme est plus complexe que les autres,La complexit√© de l'algorithme r√©side dans le processus \"it√©ratif\" de construction de l'arbre. L'algorithme doit s√©lectionner les caract√©ristiques qui permettent de s√©parer au mieux les donn√©es en fonction de la variable cible en utilisant une fonction de mesure d'impurt√© comme l'entropie ou le coefficient de Gini.\n",
    "\n",
    "- En ce qui concerne les temps d'ex√©cution des tests, le classement est diff√©rent. Le temps de test le plus court est pour l'arbre de d√©cision, suivi de la r√©gression logistique, du Naive Bayes Multinomial et du Naive Bayes Gaussien. Par cons√©quent, le classement de temps de test, du plus court au plus long, serait : arbre de d√©cision, r√©gression logistique, Naive Bayes Multinomial, Naive Bayes Gaussien.\n",
    "\n",
    "\n",
    "- En ce qui concerne le temps de test, les algorithmes qui ont produit les r√©sultats les plus rapides ont tendance √† √™tre ceux qui sont les plus sp√©cialis√©s pour les donn√©es de texte, comme l'arbre de d√©cision et la r√©gression logistique. Les Naive Bayes sont √©galement assez rapides √† tester car leur calcul de probabilit√© est simple, mais le Naive Bayes Gaussien a tendance √† √™tre plus lent car il doit estimer la moyenne et la variance pour chaque caract√©ristique.\n",
    "    - **Arbre de d√©cision** :  m√™me si la construction de l'arbre de d√©cision peut prendre du temps pendant l'entra√Ænement. Cependant, lors des tests, le temps d'ex√©cution de l'arbre de d√©cision est plus court que celui des autres algorithmes. Cela est d√ª au fait que pour chaque message, l'arbre de d√©cision effectue une seule descente dans l'arbre pour d√©terminer la classe pr√©dite, ce qui est g√©n√©ralement plus rapide que les calculs n√©cessaires pour les autres algorithmes.\n",
    "    - **R√©gression Logistique**: La raison pour laquelle la r√©gression logistique est relativement rapide pour les tests est que les pr√©dictions sont bas√©es sur une simple multiplication matricielle et une addition, ce qui est un calcul rapide. Une fois que les coefficients optimaux ont √©t√© trouv√©s pendant l'entra√Ænement (implique de r√©soudre un syst√®me dequations lin√©aires qui necessite des calculs it√©ratifs pour converger) , alors que les tests sont simplement une question de multiplication matricielle et d'addition.\n",
    "    - **Naive Bayes Multinomial**: lors du test, le mod√®le doit pr√©dire la classe pour chaque nouveau message en calculant la probabilit√© de chaque classe pour ce message. Cela implique de calculer les probabilit√©s conditionnelles pour chaque mot dans le message, ce qui peut √™tre co√ªteux en termes de temps de calcul, surtout si le nombre de caract√©ristiques (mots) est √©lev√©. Par cons√©quent, le Naive Bayes Multinomial peut √™tre lent lors des tests, m√™me s'il est rapide lors de l'entra√Ænement.\n",
    "    - **Naive Bayes Gaussien**: Le Naive Bayes Gaussien peut devenir tr√®s lent lors des tests car il doit estimer la moyenne et la variance pour chaque caract√©ristique de l'ensemble de test. Cela peut √™tre co√ªteux en termes de temps de calcul, surtout si le nombre de caract√©ristiques est important. De plus, le Naive Bayes Gaussien suppose que les donn√©es suivent une distribution normale, ce qui peut ne pas √™tre vrai pour toutes les caract√©ristiques. Dans ce cas, les estimations des moyennes et des variances peuvent ne pas √™tre pr√©cises, ce qui peut √©galement affecter la performance de l'algorithme lors des tests.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II.3.2. Qualit√© de pr√©diction\n",
    "\n",
    "Comment chaque algorithme performe sur le dataset de test dans le cas de d√©tection de spams (spam: est la classe positive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Algorithme</th>\n",
       "      <th>Rappel</th>\n",
       "      <th>Precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Naive Bayes Multinomial (NBM)</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.987179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Naive Bayes Gaussien (NBG)</td>\n",
       "      <td>0.891566</td>\n",
       "      <td>0.616667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Regression logistique (RL)</td>\n",
       "      <td>0.855422</td>\n",
       "      <td>0.986111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arbre de decision (AD)</td>\n",
       "      <td>0.873494</td>\n",
       "      <td>0.895062</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Algorithme    Rappel  Precision\n",
       "0  Naive Bayes Multinomial (NBM)  0.927711   0.987179\n",
       "1     Naive Bayes Gaussien (NBG)  0.891566   0.616667\n",
       "2     Regression logistique (RL)  0.855422   0.986111\n",
       "3         Arbre de decision (AD)  0.873494   0.895062"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({\n",
    "    'Algorithme' : algo_noms,\n",
    "    'Rappel'     : rappel   ,\n",
    "    'Precision'  : precision\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: Analyser les r√©sultats**\n",
    "\n",
    "On remarque que Naive Bayes surpasse la r√©gression logistique pour la d√©tection de spams. \n",
    "- Est-ce que ceci preuve que Naive Bayes est meilleur que les autres algorithmes sur n'importe quel probl√®me ?\n",
    "- Est-ce que ceci preuve que Naive Bayes peut donner de meilleurs r√©sultats que les autres algorithmes sur des probl√®mes similaires ?\n",
    "- Pourquoi le mod√®le gaussien est moins performant que le multinomial en se basant sur la nature des deux algorithmes ?\n",
    "- Pourquoi le mod√®le gaussien est moins performant que le multinomial en se basant sur la nature du probleme/donnees ?\n",
    "\n",
    "**R√©ponse**\n",
    "\n",
    "- Non. La sup√©riorit√© du Naive Bayes sur la r√©gression logistique dans la d√©tection de spams ne prouve pas n√©cessairement que le Naive Bayes est meilleur que les autres algorithmes sur n'importe quel probl√®me. Les performances d'un algorithme d√©pendent de la nature des donn√©es et du probl√®me √† r√©soudre.\n",
    "\n",
    "\n",
    "- Non. Les r√©sultats obtenus dans cette √©tude montrent que Naive  a produit des performances sup√©rieures √† celles des autres algorithmes pour la t√¢che de d√©tection de spam. Cependant, cela ne signifie pas que Naive Bayes sera toujours le meilleur algorithme pour tous les probl√®mes similaires. Les performances des algorithmes d√©pendent de nombreux facteurs tels que la nature des donn√©es, les param√®tres d'entr√©e, la taille de l'ensemble de donn√©es.\n",
    "\n",
    "\n",
    "- Le mod√®le gaussien est moins performant que le multinomial car les donn√©es textuelles ont une distribution multinomiale, ce qui signifie que chaque caract√©ristique peut prendre un nombre fini de valeurs discr√®tes. Le mod√®le gaussien, quant √† lui, suppose que les caract√©ristiques ont une distribution normale (ou gaussienne). Cette hypoth√®se n'est pas toujours valide pour les donn√©es textuelles, ce qui peut affecter n√©gativement les performances du mod√®le gaussien.\n",
    "\n",
    "    De plus, le mod√®le gaussien n√©cessite l'estimation de la moyenne et de la variance pour chaque caract√©ristique, ce qui peut √™tre co√ªteux en termes de temps de traitement et de m√©moire. En revanche, le mod√®le multinomial est plus simple et plus rapide √† calculer car il ne n√©cessite que le compte des occurrences de chaque mot dans chaque document, ce qui est plus appropri√© pour les donn√©es textuelles.\n",
    "    En ce qui concerne la nature des deux algorithmes, le mod√®le multinomial est mieux adapt√© aux donn√©es textuelles car il peut prendre en compte la fr√©quence d'apparition des mots dans les documents, alors que le mod√®le gaussien suppose que les caract√©ristiques sont continues et ont une distribution normale, ce qui peut ne pas √™tre le cas pour les donn√©es textuelles.\n",
    "    \n",
    "    \n",
    "- Le mod√®le gaussien est moins performant que le multinomial dans ce cas particulier en raison de la nature des donn√©es de spam. Les caract√©ristiques utilis√©es pour d√©crire les courriels (par exemple, la fr√©quence des mots) sont mieux d√©crites par une distribution multinomiale, car elles d√©crivent la fr√©quence d'occurrence de chaque mot dans un texte. En revanche, le mod√®le gaussien suppose que les donn√©es sont distribu√©es selon une loi normale, ce qui peut ne pas √™tre le cas pour les caract√©ristiques des courriels de spam. Par cons√©quent, le mod√®le gaussien peut ne pas √™tre en mesure de mod√©liser correctement la distribution des donn√©es, ce qui peut entra√Æner des performances inf√©rieures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  _____    __                                              _               \n",
      " |_   _|  / _|                                            | |              \n",
      "   | |   | |_     _   _    ___    _   _      __ _    ___  | |_             \n",
      "   | |   |  _|   | | | |  / _ \\  | | | |    / _` |  / _ \\ | __|            \n",
      "  _| |_  | |     | |_| | | (_) | | |_| |   | (_| | |  __/ | |_             \n",
      " |_____| |_|      \\__, |  \\___/   \\__,_|    \\__, |  \\___|  \\__|            \n",
      "                   __/ |                     __/ |                         \n",
      "                  |___/                     |___/                          \n",
      "  _     _       _            __                                            \n",
      " | |   | |     (_)          / _|                 _                         \n",
      " | |_  | |__    _   ___    | |_    __ _   _ __  (_)                        \n",
      " | __| | '_ \\  | | / __|   |  _|  / _` | | '__|                            \n",
      " | |_  | | | | | | \\__ \\   | |   | (_| | | |     _                         \n",
      "  \\__| |_| |_| |_| |___/   |_|    \\__,_| |_|    ( )                        \n",
      "                                                |/                         \n",
      "                                                                           \n",
      "                                                                           \n",
      "                                                                           \n",
      "  _   _    ___    _   _      __ _   _ __    ___                            \n",
      " | | | |  / _ \\  | | | |    / _` | | '__|  / _ \\                           \n",
      " | |_| | | (_) | | |_| |   | (_| | | |    |  __/                           \n",
      "  \\__, |  \\___/   \\__,_|    \\__,_| |_|     \\___|                           \n",
      "   __/ |                                                                   \n",
      "  |___/                                                                    \n",
      "                    _                                                __    \n",
      "                   | |                                            _  \\ \\   \n",
      "  _ __     ___     | |__    _   _   _ __ ___     __ _   _ __     (_)  | |  \n",
      " | '_ \\   / _ \\    | '_ \\  | | | | | '_ ` _ \\   / _` | | '_ \\         | |  \n",
      " | | | | | (_) |   | | | | | |_| | | | | | | | | (_| | | | | |    _   | |  \n",
      " |_| |_|  \\___/    |_| |_|  \\__,_| |_| |_| |_|  \\__,_| |_| |_|   (_)  | |  \n",
      "                                                                     /_/   \n",
      "                                                                           \n"
     ]
    }
   ],
   "source": [
    "print(\"  _____    __                                              _               \")\n",
    "print(\" |_   _|  / _|                                            | |              \")\n",
    "print(\"   | |   | |_     _   _    ___    _   _      __ _    ___  | |_             \")\n",
    "print(\"   | |   |  _|   | | | |  / _ \\  | | | |    / _` |  / _ \\ | __|            \")\n",
    "print(\"  _| |_  | |     | |_| | | (_) | | |_| |   | (_| | |  __/ | |_             \")\n",
    "print(\" |_____| |_|      \\__, |  \\___/   \\__,_|    \\__, |  \\___|  \\__|            \")\n",
    "print(\"                   __/ |                     __/ |                         \")\n",
    "print(\"                  |___/                     |___/                          \")\n",
    "print(\"  _     _       _            __                                            \")\n",
    "print(\" | |   | |     (_)          / _|                 _                         \")\n",
    "print(\" | |_  | |__    _   ___    | |_    __ _   _ __  (_)                        \")\n",
    "print(\" | __| | '_ \\  | | / __|   |  _|  / _` | | '__|                            \")\n",
    "print(\" | |_  | | | | | | \\__ \\   | |   | (_| | | |     _                         \")\n",
    "print(\"  \\__| |_| |_| |_| |___/   |_|    \\__,_| |_|    ( )                        \")\n",
    "print(\"                                                |/                         \")\n",
    "print(\"                                                                           \")\n",
    "print(\"                                                                           \")\n",
    "print(\"                                                                           \")\n",
    "print(\"  _   _    ___    _   _      __ _   _ __    ___                            \")\n",
    "print(\" | | | |  / _ \\  | | | |    / _` | | '__|  / _ \\                           \")\n",
    "print(\" | |_| | | (_) | | |_| |   | (_| | | |    |  __/                           \")\n",
    "print(\"  \\__, |  \\___/   \\__,_|    \\__,_| |_|     \\___|                           \")\n",
    "print(\"   __/ |                                                                   \")\n",
    "print(\"  |___/                                                                    \")\n",
    "print(\"                    _                                                __    \")\n",
    "print(\"                   | |                                            _  \\ \\   \")\n",
    "print(\"  _ __     ___     | |__    _   _   _ __ ___     __ _   _ __     (_)  | |  \")\n",
    "print(\" | '_ \\   / _ \\    | '_ \\  | | | | | '_ ` _ \\   / _` | | '_ \\         | |  \")\n",
    "print(\" | | | | | (_) |   | | | | | |_| | | | | | | | | (_| | | | | |    _   | |  \")\n",
    "print(\" |_| |_|  \\___/    |_| |_|  \\__,_| |_| |_| |_|  \\__,_| |_| |_|   (_)  | |  \")\n",
    "print(\"                                                                     /_/   \")\n",
    "print(\"                                                                           \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
